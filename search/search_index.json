{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyWarm A cleaner way to build neural networks for PyTorch. Examples | Tutorial | API reference Introduction PyWarm is a lightweight, high-level neural network construction API for PyTorch. It enables defining all parts of NNs in the functional way. With PyWarm, you can put all network data flow logic in the forward() method of your model, without the need to define children modules in the __init__() method and then call it again in the forward() . This result in a much more readable model definition in fewer lines of code. PyWarm only aims to simplify the network definition, and does not attempt to cover model training, validation or data handling. For example, a convnet for MNIST: (If needed, click the tabs to switch between Warm and Torch versions) Warm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # powered by PyWarm import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W class ConvNet ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , [ 2 , 1 , 28 , 28 ]) def forward ( self , x ): x = W . conv ( x , 20 , 5 , activation = 'relu' ) x = F . max_pool2d ( x , 2 ) x = W . conv ( x , 50 , 5 , activation = 'relu' ) x = F . max_pool2d ( x , 2 ) x = x . view ( - 1 , 800 ) x = W . linear ( x , 500 , activation = 'relu' ) x = W . linear ( x , 10 ) return F . log_softmax ( x , dim = 1 ) Torch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # vanilla PyTorch version, taken from # pytorch tutorials/beginner_source/blitz/neural_networks_tutorial.py import torch.nn as nn import torch.nn.functional as F class ConvNet ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 , 1 ) self . conv2 = nn . Conv2d ( 20 , 50 , 5 , 1 ) self . fc1 = nn . Linear ( 4 * 4 * 50 , 500 ) self . fc2 = nn . Linear ( 500 , 10 ) def forward ( self , x ): x = F . relu ( self . conv1 ( x )) x = F . max_pool2d ( x , 2 , 2 ) x = F . relu ( self . conv2 ( x )) x = F . max_pool2d ( x , 2 , 2 ) x = x . view ( - 1 , 4 * 4 * 50 ) x = F . relu ( self . fc1 ( x )) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) A couple of things you may have noticed: First of all, in the PyWarm version, the entire network definition and data flow logic resides in the forward() method. You don't have to look up and down repeatedly to understand what self.conv1 , self.fc1 etc. is doing. You do not need to track and specify in_channels (or in_features , etc.) for network layers. PyWarm can infer the information for you. e.g. # Warm x = W . conv ( x , 20 , 5 , activation = 'relu' ) x = W . conv ( x , 50 , 5 , activation = 'relu' ) # Torch self . conv1 = nn . Conv2d ( 1 , 20 , 5 , 1 ) self . conv2 = nn . Conv2d ( 20 , 50 , 5 , 1 ) One unified W.conv for all 1D, 2D, and 3D cases. Fewer things to keep track of! activation='relu' . All warm.functional APIs accept an optional activation keyword, which is basically equivalent to F.relu(W.conv(...)) . For deeper neural networks, see additional examples . Installation pip3 install pywarm Quick start: 30 seconds to PyWarm If you already have experinces with PyTorch, using PyWarm is very straightforward: First, import PyWarm in you model file: import warm import warm.functional as W Second, remove child module definitions in the model's __init__() method. In stead, use W.conv , W.linear ... etc. in the model's forward() method, just like how you would use torch nn functional F.max_pool2d , F.relu ... etc. For example, instead of writing: # Torch class MyModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( in_channels , out_channels , kernel_size ) # other child module definitions def forward ( self , x ): x = self . conv1 ( x ) # more forward steps You can now write in the warm way: # Warm class MyWarmModule ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , input_shape_or_data ) def forward ( self , x ): x = W . conv ( x , out_channels , kernel_size ) # no in_channels needed # more forward steps Finally, don't forget to warmify the model by adding warm.up(self, input_shape_or_data) at the end of the model's __init__() method. You need to supply input_shape_or_data , which is either a tensor of input data, or just its shape, e.g. [2, 1, 28, 28] for MNIST inputs. The model is now ready to use, just like any other PyTorch models. Check out the tutorial and examples if you want to learn more! Testing Clone the repository first, then cd pywarm pytest -v Documentation Documentations are generated using the excellent Portray package. Examples Tutorial API reference","title":"Home"},{"location":"#pywarm","text":"A cleaner way to build neural networks for PyTorch. Examples | Tutorial | API reference","title":"PyWarm"},{"location":"#introduction","text":"PyWarm is a lightweight, high-level neural network construction API for PyTorch. It enables defining all parts of NNs in the functional way. With PyWarm, you can put all network data flow logic in the forward() method of your model, without the need to define children modules in the __init__() method and then call it again in the forward() . This result in a much more readable model definition in fewer lines of code. PyWarm only aims to simplify the network definition, and does not attempt to cover model training, validation or data handling. For example, a convnet for MNIST: (If needed, click the tabs to switch between Warm and Torch versions) Warm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # powered by PyWarm import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W class ConvNet ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , [ 2 , 1 , 28 , 28 ]) def forward ( self , x ): x = W . conv ( x , 20 , 5 , activation = 'relu' ) x = F . max_pool2d ( x , 2 ) x = W . conv ( x , 50 , 5 , activation = 'relu' ) x = F . max_pool2d ( x , 2 ) x = x . view ( - 1 , 800 ) x = W . linear ( x , 500 , activation = 'relu' ) x = W . linear ( x , 10 ) return F . log_softmax ( x , dim = 1 ) Torch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # vanilla PyTorch version, taken from # pytorch tutorials/beginner_source/blitz/neural_networks_tutorial.py import torch.nn as nn import torch.nn.functional as F class ConvNet ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 , 1 ) self . conv2 = nn . Conv2d ( 20 , 50 , 5 , 1 ) self . fc1 = nn . Linear ( 4 * 4 * 50 , 500 ) self . fc2 = nn . Linear ( 500 , 10 ) def forward ( self , x ): x = F . relu ( self . conv1 ( x )) x = F . max_pool2d ( x , 2 , 2 ) x = F . relu ( self . conv2 ( x )) x = F . max_pool2d ( x , 2 , 2 ) x = x . view ( - 1 , 4 * 4 * 50 ) x = F . relu ( self . fc1 ( x )) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) A couple of things you may have noticed: First of all, in the PyWarm version, the entire network definition and data flow logic resides in the forward() method. You don't have to look up and down repeatedly to understand what self.conv1 , self.fc1 etc. is doing. You do not need to track and specify in_channels (or in_features , etc.) for network layers. PyWarm can infer the information for you. e.g. # Warm x = W . conv ( x , 20 , 5 , activation = 'relu' ) x = W . conv ( x , 50 , 5 , activation = 'relu' ) # Torch self . conv1 = nn . Conv2d ( 1 , 20 , 5 , 1 ) self . conv2 = nn . Conv2d ( 20 , 50 , 5 , 1 ) One unified W.conv for all 1D, 2D, and 3D cases. Fewer things to keep track of! activation='relu' . All warm.functional APIs accept an optional activation keyword, which is basically equivalent to F.relu(W.conv(...)) . For deeper neural networks, see additional examples .","title":"Introduction"},{"location":"#installation","text":"pip3 install pywarm","title":"Installation"},{"location":"#quick-start-30-seconds-to-pywarm","text":"If you already have experinces with PyTorch, using PyWarm is very straightforward: First, import PyWarm in you model file: import warm import warm.functional as W Second, remove child module definitions in the model's __init__() method. In stead, use W.conv , W.linear ... etc. in the model's forward() method, just like how you would use torch nn functional F.max_pool2d , F.relu ... etc. For example, instead of writing: # Torch class MyModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( in_channels , out_channels , kernel_size ) # other child module definitions def forward ( self , x ): x = self . conv1 ( x ) # more forward steps You can now write in the warm way: # Warm class MyWarmModule ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , input_shape_or_data ) def forward ( self , x ): x = W . conv ( x , out_channels , kernel_size ) # no in_channels needed # more forward steps Finally, don't forget to warmify the model by adding warm.up(self, input_shape_or_data) at the end of the model's __init__() method. You need to supply input_shape_or_data , which is either a tensor of input data, or just its shape, e.g. [2, 1, 28, 28] for MNIST inputs. The model is now ready to use, just like any other PyTorch models. Check out the tutorial and examples if you want to learn more!","title":"Quick start: 30 seconds to PyWarm"},{"location":"#testing","text":"Clone the repository first, then cd pywarm pytest -v","title":"Testing"},{"location":"#documentation","text":"Documentations are generated using the excellent Portray package. Examples Tutorial API reference","title":"Documentation"},{"location":"CONTRIBUTING/","text":"Contributing to PyWarm PyWarm is developed on GitHub . Please use GitHub to file Bug reports and submit pull requests. Please document and test before submissions. PyWarm is developed with Python 3.7, but has been tested to work with Python 3.6+. Coding Style For the rational behind the distinct coding style use in PyWarm, please check A Coding Style for Python .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-pywarm","text":"PyWarm is developed on GitHub . Please use GitHub to file Bug reports and submit pull requests. Please document and test before submissions. PyWarm is developed with Python 3.7, but has been tested to work with Python 3.6+.","title":"Contributing to PyWarm"},{"location":"CONTRIBUTING/#coding-style","text":"For the rational behind the distinct coding style use in PyWarm, please check A Coding Style for Python .","title":"Coding Style"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2019 blue-season Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"docs/example/","text":"PyWarm Examples ResNet A more detailed example, the ResNet18 network defined in PyWarm and vanilla PyTorch: Warm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def basic ( x , size , stride ): y = W . conv ( x , size , 3 , stride = stride , padding = 1 , bias = False ) y = W . batch_norm ( y , activation = 'relu' ) y = W . conv ( y , size , 3 , stride = 1 , padding = 1 , bias = False ) y = W . batch_norm ( y ) if y . shape [ 1 ] != x . shape [ 1 ]: # channel size mismatch, needs projection x = W . conv ( x , y . shape [ 1 ], 1 , stride = stride , bias = False ) x = W . batch_norm ( x ) y = y + x # residual shortcut connection return F . relu ( y ) def stack ( x , num_block , size , stride , block = basic ): for s in [ stride ] + [ 1 ] * ( num_block - 1 ): x = block ( x , size , s ) return x class ResNet ( nn . Module ): def __init__ ( self , block = basic , stack_spec = (( 2 , 64 , 1 ), ( 2 , 128 , 2 ), ( 2 , 256 , 2 ), ( 2 , 512 , 2 ))): super () . __init__ () self . block = block self . stack_spec = stack_spec warm . up ( self , [ 2 , 3 , 32 , 32 ]) def forward ( self , x ): y = W . conv ( x , 64 , 7 , stride = 2 , padding = 3 , bias = False ) y = W . batch_norm ( y , activation = 'relu' ) y = F . max_pool2d ( y , 3 , stride = 2 , padding = 1 ) for spec in self . stack_spec : y = stack ( y , * spec , block = self . block ) y = F . adaptive_avg_pool2d ( y , 1 ) y = torch . flatten ( y , 1 ) y = W . linear ( y , 1000 ) return y resnet18 = ResNet () Torch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 # code based on torchvision/models/resnet.py import torch import torch.nn as nn import torch.nn.functional as F def conv3x3 ( size_in , size_out , stride = 1 ): return nn . Conv2d ( size_in , size_out , kernel_size = 3 , stride = stride , padding = 1 , groups = 1 , bias = False , dilation = 1 , ) def conv1x1 ( size_in , size_out , stride = 1 ): return nn . Conv2d ( size_in , size_out , kernel_size = 1 , stride = stride , padding = 0 , groups = 1 , bias = False , dilation = 1 , ) class BasicBlock ( nn . Module ): expansion = 1 def __init__ ( self , size_in , size_out , stride = 1 , downsample = None ): super () . __init__ () self . conv1 = conv3x3 ( size_in , size_out , stride ) self . bn1 = nn . BatchNorm2d ( size_out ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( size_out , size_out ) self . bn2 = nn . BatchNorm2d ( size_out ) self . downsample = downsample def forward ( self , x ): identity = x y = self . conv1 ( x ) y = self . bn1 ( y ) y = self . relu ( y ) y = self . conv2 ( y ) y = self . bn2 ( y ) if self . downsample is not None : identity = self . downsample ( x ) y += identity y = self . relu ( y ) return y class ResNet ( nn . Module ): def __init__ ( self , block = BasicBlock , num_block = [ 2 , 2 , 2 , 2 ]): super () . __init__ () self . size_in = 64 self . conv1 = nn . Conv2d ( 3 , self . size_in , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ) self . bn1 = nn . BatchNorm2d ( self . size_in ) self . relu = nn . ReLU ( inplace = True ) self . maxpool = nn . MaxPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ) self . stack1 = self . _make_stack ( block , 64 , num_block [ 0 ], 1 ) self . stack2 = self . _make_stack ( block , 128 , num_block [ 1 ], 2 ) self . stack3 = self . _make_stack ( block , 256 , num_block [ 2 ], 2 ) self . stack4 = self . _make_stack ( block , 512 , num_block [ 3 ], 2 ) self . avg_pool = nn . AdaptiveAvgPool2d ( 1 ) self . fc = nn . Linear ( 512 , 1000 ) def _make_stack ( self , block , size_out , num_blocks , stride ): downsample = None if stride != 1 or self . size_in != size_out : downsample = nn . Sequential ( conv1x1 ( self . size_in , size_out , stride ), nn . BatchNorm2d ( size_out ), ) stacks = [] for stride in strides : stacks . append ( block ( self . size_in , size_out , stride , downsample )) self . size_in = size_out return nn . Sequential ( * stacks ) def forward ( self , x ): y = self . conv1 ( x ) y = self . bn1 ( y ) y = self . relu ( y ) y = self . maxpool ( y ) y = self . stack1 ( y ) y = self . stack2 ( y ) y = self . stack3 ( y ) y = self . stack4 ( y ) y = self . avg_pool ( y ) y = torch . flatten ( y , 1 ) y = self . fc ( y ) return y resnet18 = ResNet () The PyWarm version significantly reduces self-repititions of code as in the vanilla PyTorch version. Note that when warming the model via warm.up(self, [2, 3, 32, 32]) We set the first Batch dimension to 2 because the model uses batch_norm , which will not work when Batch is 1. MobileNet Warm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def conv_bn_relu ( x , size , stride = 1 , expand = 1 , kernel = 3 , groups = 1 ): x = W . conv ( x , size , kernel , padding = ( kernel - 1 ) // 2 , stride = stride , groups = groups , bias = False , ) return W . batch_norm ( x , activation = 'relu6' ) def bottleneck ( x , size_out , stride , expand ): size_in = x . shape [ 1 ] size_mid = size_in * expand y = conv_bn_relu ( x , size_mid , kernel = 1 ) if expand > 1 else x y = conv_bn_relu ( y , size_mid , stride , kernel = 3 , groups = size_mid ) y = W . conv ( y , size_out , kernel = 1 , bias = False ) y = W . batch_norm ( y ) if stride == 1 and size_in == size_out : y += x # residual shortcut return y def conv1x1 ( x , * arg ): return conv_bn_relu ( x , * arg , kernel = 1 ) def pool ( x , * arg ): return x . mean ([ 2 , 3 ]) def classify ( x , size , * arg ): x = W . dropout ( x , rate = 0.2 ) return W . linear ( x , size ) default_spec = ( ( None , 32 , 1 , 2 , conv_bn_relu ), # t, c, n, s, operator ( 1 , 16 , 1 , 1 , bottleneck ), ( 6 , 24 , 2 , 2 , bottleneck ), ( 6 , 32 , 3 , 2 , bottleneck ), ( 6 , 64 , 4 , 2 , bottleneck ), ( 6 , 96 , 3 , 1 , bottleneck ), ( 6 , 160 , 3 , 2 , bottleneck ), ( 6 , 320 , 1 , 1 , bottleneck ), ( None , 1280 , 1 , 1 , conv1x1 ), ( None , None , 1 , None , pool ), ( None , 1000 , 1 , None , classify ), ) class MobileNetV2 ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , [ 2 , 3 , 224 , 224 ]) def forward ( self , x ): for t , c , n , s , op in default_spec : for i in range ( n ): stride = s if i == 0 else 1 x = op ( x , c , stride , t ) return x net = MobileNetV2 () Torch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 # code based on torchvision/models/mobilenet.py import torch import torch.nn as nn import torch.nn.functional as F class ConvBNReLU ( nn . Sequential ): def __init__ ( self , in_planes , out_planes , kernel_size = 3 , stride = 1 , groups = 1 ): padding = ( kernel_size - 1 ) // 2 super ( ConvBNReLU , self ) . __init__ ( nn . Conv2d ( in_planes , out_planes , kernel_size , stride , padding , groups = groups , bias = False ), nn . BatchNorm2d ( out_planes ), nn . ReLU6 ( inplace = True ), ) class BottleNeck ( nn . Module ): def __init__ ( self , inp , oup , stride , expand_ratio ): super () . __init__ () self . stride = stride assert stride in [ 1 , 2 ] hidden_dim = int ( round ( inp * expand_ratio )) self . use_res_connect = self . stride == 1 and inp == oup layers = [] if expand_ratio != 1 : layers . append ( ConvBNReLU ( inp , hidden_dim , kernel_size = 1 )) layers . extend ([ ConvBNReLU ( hidden_dim , hidden_dim , stride = stride , groups = hidden_dim ), nn . Conv2d ( hidden_dim , oup , 1 , 1 , 0 , bias = False ), nn . BatchNorm2d ( oup ), ]) self . conv = nn . Sequential ( * layers ) def forward ( self , x ): if self . use_res_connect : return x + self . conv ( x ) else : return self . conv ( x ) default_spec = [ [ 1 , 16 , 1 , 1 ], # t, c, n, s [ 6 , 24 , 2 , 2 ], [ 6 , 32 , 3 , 2 ], [ 6 , 64 , 4 , 2 ], [ 6 , 96 , 3 , 1 ], [ 6 , 160 , 3 , 2 ], [ 6 , 320 , 1 , 1 ], ] class MobileNetV2 ( nn . Module ): def __init__ ( self ): super () . __init__ () input_channel = 32 last_channel = 1280 features = [ ConvBNReLU ( 3 , input_channel , stride = 2 )] for t , c , n , s in default_spec : output_channel = c for i in range ( n ): stride = s if i == 0 else 1 features . append ( BottleNeck ( input_channel , output_channel , stride , expand_ratio = t )) input_channel = output_channel features . append ( ConvBNReLU ( input_channel , last_channel , kernel_size = 1 )) self . features = nn . Sequential ( * features ) self . classifier = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( last_channel , 1000 ), ) def forward ( self , x ): x = self . features ( x ) x = x . mean ([ 2 , 3 ]) x = self . classifier ( x ) return x net = MobileNetV2 () Transformer \"\"\" The Transformer model from paper Attention is all you need. The Transformer instance accepts two inputs: x is Tensor with shape (Batch, Channel, LengthX). usually a source sequence from embedding (in such cases, Channel equals the embedding size). y is Tensor with shape (Batch, Channel, lengthY). usually a target sequence, also from embedding. **kw is passed down to inner components. \"\"\" import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def multi_head_attention ( x , y = None , num_head = 8 , dropout = 0.1 , mask = None , ** kw ): def split_heads ( t ): return t . reshape ( batch , num_head , size // num_head , t . shape [ - 1 ]) def merge_heads ( t ): return t . reshape ( batch , - 1 , t . shape [ - 1 ]) if y is None : y = x # self attention batch , size = x . shape [: 2 ] assert size % num_head == 0 , 'num_head must be a divisor of size.' assert y . shape [: 2 ] == x . shape [: 2 ], 'The first 2 dims of x, y must match.' q = W . linear ( x , size ) # query k = W . linear ( y , size ) # key v = W . linear ( y , size ) # value q = split_heads ( q ) k = split_heads ( k ) v = split_heads ( v ) q *= ( size // num_head ) ** ( - 0.5 ) a = q . transpose ( 2 , 3 ) . contiguous () . matmul ( k ) # attention weights if mask is not None : a += mask a = F . softmax ( a , dim =- 1 ) a = W . dropout ( a , dropout ) x = v . matmul ( a . transpose ( 2 , 3 ) . contiguous ()) x = merge_heads ( x ) return W . linear ( x , size ) def feed_forward ( x , size_ff = 2048 , dropout = 0.1 , ** kw ): y = W . linear ( x , size_ff , activation = 'relu' ) y = W . dropout ( y , dropout ) return W . linear ( y , x . shape [ 1 ]) def residual_add ( x , layer , dropout = 0.1 , ** kw ): y = W . layer_norm ( x ) y = layer ( y , ** kw ) y = W . dropout ( y , dropout ) return x + y def encoder ( x , num_encoder = 6 , ** kw ): for i in range ( num_encoder ): x = residual_add ( x , multi_head_attention , ** kw ) x = residual_add ( x , feed_forward , ** kw ) return W . layer_norm ( x ) def decoder ( x , y , num_decoder = 6 , mask_x = None , mask_y = None , ** kw ): for i in range ( num_decoder ): y = residual_add ( y , multi_head_attention , mask = mask_y , ** kw ) y = residual_add ( x , multi_head_attention , y = y , mask = mask_x , ** kw ) y = residual_add ( y , feed_forward , ** kw ) return W . layer_norm ( y ) def transformer ( x , y , ** kw ): x = encoder ( x , ** kw ) x = decoder ( x , y , ** kw ) return x class Transformer ( nn . Module ): def __init__ ( self , * shape , ** kw ): super () . __init__ () self . kw = kw warm . up ( self , * shape ) def forward ( self , x , y ): return transformer ( x , y , ** self . kw ) EfficientNet For a brief overview, check the blog post . \"\"\" EfficientNet model from https://arxiv.org/abs/1905.11946 \"\"\" import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def swish ( x ): return x * torch . sigmoid ( x ) def squeeze_excitation ( x , size_se ): if size_se == 0 : return x size_in = x . shape [ 1 ] x = F . adaptive_avg_pool2d ( x , 1 ) x = W . conv ( x , size_se , 1 , activation = swish ) return W . conv ( x , size_in , 1 , activation = swish ) def drop_connect ( x , rate ): if rate == 0 : return x rate = 1.0 - rate drop_mask = rate + torch . rand ([ x . shape [ 0 ], 1 , 1 , 1 ], device = x . device , requires_grad = False ) return x / rate * drop_mask . floor () def conv_pad_same ( x , size , kernel = 1 , stride = 1 , ** kw ): \"\"\" Same padding so that out_size*stride == in_size. \"\"\" pad = 0 if kernel != 1 or stride != 1 : in_size , s , k = [ torch . as_tensor ( v ) for v in ( x . shape [ 2 :], stride , kernel )] pad = torch . max ((( in_size + s - 1 ) // s - 1 ) * s + k - in_size , torch . tensor ( 0 )) left , right = pad // 2 , pad - pad // 2 if torch . all ( left == right ): pad = tuple ( left . tolist ()) else : left , right = left . tolist (), right . tolist () pad = sum ( zip ( left [:: - 1 ], right [:: - 1 ]), ()) x = F . pad ( x , pad ) pad = 0 return W . conv ( x , size , kernel , stride = stride , padding = pad , ** kw ) def conv_bn_act ( x , size , kernel = 1 , stride = 1 , groups = 1 , bias = False , eps = 1e-3 , momentum = 1e-2 , act = swish ): x = conv_pad_same ( x , size , kernel , stride = stride , groups = groups , bias = bias ) return W . batch_norm ( x , eps = eps , momentum = momentum , activation = act ) def mb_block ( x , size_out , expand = 1 , kernel = 1 , stride = 1 , se_ratio = 0.25 , dc_ratio = 0.2 ): \"\"\" Mobilenet Bottleneck Block. \"\"\" size_in = x . shape [ 1 ] size_mid = size_in * expand y = conv_bn_act ( x , size_mid , 1 ) if expand > 1 else x y = conv_bn_act ( y , size_mid , kernel , stride = stride , groups = size_mid ) y = squeeze_excitation ( y , int ( size_in * se_ratio )) y = conv_bn_act ( y , size_out , 1 , act = None ) if stride == 1 and size_in == size_out : y = drop_connect ( y , dc_ratio ) y += x return y spec_b0 = ( # size, expand, kernel, stride, repeat, squeeze_excitation, drop_connect ( 16 , 1 , 3 , 1 , 1 , 0.25 , 0.2 ), ( 24 , 6 , 3 , 2 , 2 , 0.25 , 0.2 ), ( 40 , 6 , 5 , 2 , 2 , 0.25 , 0.2 ), ( 80 , 6 , 3 , 2 , 3 , 0.25 , 0.2 ), ( 112 , 6 , 5 , 1 , 3 , 0.25 , 0.2 ), ( 192 , 6 , 5 , 2 , 4 , 0.25 , 0.2 ), ( 320 , 6 , 3 , 1 , 1 , 0.25 , 0.2 ), ) class WarmEfficientNet ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , [ 2 , 3 , 32 , 32 ]) def forward ( self , x ): x = conv_bn_act ( x , 32 , kernel = 3 , stride = 2 ) for size , expand , kernel , stride , repeat , se , dc in spec_b0 : for i in range ( repeat ): stride = stride if i == 0 else 1 x = mb_block ( x , size , expand , kernel , stride , se , dc ) x = conv_bn_act ( x , 1280 ) x = F . adaptive_avg_pool2d ( x , 1 ) x = W . dropout ( x , 0.2 ) x = x . view ( x . shape [ 0 ], - 1 ) x = W . linear ( x , 1000 ) return x","title":"Example"},{"location":"docs/example/#pywarm-examples","text":"","title":"PyWarm Examples"},{"location":"docs/example/#resnet","text":"A more detailed example, the ResNet18 network defined in PyWarm and vanilla PyTorch: Warm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def basic ( x , size , stride ): y = W . conv ( x , size , 3 , stride = stride , padding = 1 , bias = False ) y = W . batch_norm ( y , activation = 'relu' ) y = W . conv ( y , size , 3 , stride = 1 , padding = 1 , bias = False ) y = W . batch_norm ( y ) if y . shape [ 1 ] != x . shape [ 1 ]: # channel size mismatch, needs projection x = W . conv ( x , y . shape [ 1 ], 1 , stride = stride , bias = False ) x = W . batch_norm ( x ) y = y + x # residual shortcut connection return F . relu ( y ) def stack ( x , num_block , size , stride , block = basic ): for s in [ stride ] + [ 1 ] * ( num_block - 1 ): x = block ( x , size , s ) return x class ResNet ( nn . Module ): def __init__ ( self , block = basic , stack_spec = (( 2 , 64 , 1 ), ( 2 , 128 , 2 ), ( 2 , 256 , 2 ), ( 2 , 512 , 2 ))): super () . __init__ () self . block = block self . stack_spec = stack_spec warm . up ( self , [ 2 , 3 , 32 , 32 ]) def forward ( self , x ): y = W . conv ( x , 64 , 7 , stride = 2 , padding = 3 , bias = False ) y = W . batch_norm ( y , activation = 'relu' ) y = F . max_pool2d ( y , 3 , stride = 2 , padding = 1 ) for spec in self . stack_spec : y = stack ( y , * spec , block = self . block ) y = F . adaptive_avg_pool2d ( y , 1 ) y = torch . flatten ( y , 1 ) y = W . linear ( y , 1000 ) return y resnet18 = ResNet () Torch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 # code based on torchvision/models/resnet.py import torch import torch.nn as nn import torch.nn.functional as F def conv3x3 ( size_in , size_out , stride = 1 ): return nn . Conv2d ( size_in , size_out , kernel_size = 3 , stride = stride , padding = 1 , groups = 1 , bias = False , dilation = 1 , ) def conv1x1 ( size_in , size_out , stride = 1 ): return nn . Conv2d ( size_in , size_out , kernel_size = 1 , stride = stride , padding = 0 , groups = 1 , bias = False , dilation = 1 , ) class BasicBlock ( nn . Module ): expansion = 1 def __init__ ( self , size_in , size_out , stride = 1 , downsample = None ): super () . __init__ () self . conv1 = conv3x3 ( size_in , size_out , stride ) self . bn1 = nn . BatchNorm2d ( size_out ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( size_out , size_out ) self . bn2 = nn . BatchNorm2d ( size_out ) self . downsample = downsample def forward ( self , x ): identity = x y = self . conv1 ( x ) y = self . bn1 ( y ) y = self . relu ( y ) y = self . conv2 ( y ) y = self . bn2 ( y ) if self . downsample is not None : identity = self . downsample ( x ) y += identity y = self . relu ( y ) return y class ResNet ( nn . Module ): def __init__ ( self , block = BasicBlock , num_block = [ 2 , 2 , 2 , 2 ]): super () . __init__ () self . size_in = 64 self . conv1 = nn . Conv2d ( 3 , self . size_in , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ) self . bn1 = nn . BatchNorm2d ( self . size_in ) self . relu = nn . ReLU ( inplace = True ) self . maxpool = nn . MaxPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ) self . stack1 = self . _make_stack ( block , 64 , num_block [ 0 ], 1 ) self . stack2 = self . _make_stack ( block , 128 , num_block [ 1 ], 2 ) self . stack3 = self . _make_stack ( block , 256 , num_block [ 2 ], 2 ) self . stack4 = self . _make_stack ( block , 512 , num_block [ 3 ], 2 ) self . avg_pool = nn . AdaptiveAvgPool2d ( 1 ) self . fc = nn . Linear ( 512 , 1000 ) def _make_stack ( self , block , size_out , num_blocks , stride ): downsample = None if stride != 1 or self . size_in != size_out : downsample = nn . Sequential ( conv1x1 ( self . size_in , size_out , stride ), nn . BatchNorm2d ( size_out ), ) stacks = [] for stride in strides : stacks . append ( block ( self . size_in , size_out , stride , downsample )) self . size_in = size_out return nn . Sequential ( * stacks ) def forward ( self , x ): y = self . conv1 ( x ) y = self . bn1 ( y ) y = self . relu ( y ) y = self . maxpool ( y ) y = self . stack1 ( y ) y = self . stack2 ( y ) y = self . stack3 ( y ) y = self . stack4 ( y ) y = self . avg_pool ( y ) y = torch . flatten ( y , 1 ) y = self . fc ( y ) return y resnet18 = ResNet () The PyWarm version significantly reduces self-repititions of code as in the vanilla PyTorch version. Note that when warming the model via warm.up(self, [2, 3, 32, 32]) We set the first Batch dimension to 2 because the model uses batch_norm , which will not work when Batch is 1.","title":"ResNet"},{"location":"docs/example/#mobilenet","text":"Warm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def conv_bn_relu ( x , size , stride = 1 , expand = 1 , kernel = 3 , groups = 1 ): x = W . conv ( x , size , kernel , padding = ( kernel - 1 ) // 2 , stride = stride , groups = groups , bias = False , ) return W . batch_norm ( x , activation = 'relu6' ) def bottleneck ( x , size_out , stride , expand ): size_in = x . shape [ 1 ] size_mid = size_in * expand y = conv_bn_relu ( x , size_mid , kernel = 1 ) if expand > 1 else x y = conv_bn_relu ( y , size_mid , stride , kernel = 3 , groups = size_mid ) y = W . conv ( y , size_out , kernel = 1 , bias = False ) y = W . batch_norm ( y ) if stride == 1 and size_in == size_out : y += x # residual shortcut return y def conv1x1 ( x , * arg ): return conv_bn_relu ( x , * arg , kernel = 1 ) def pool ( x , * arg ): return x . mean ([ 2 , 3 ]) def classify ( x , size , * arg ): x = W . dropout ( x , rate = 0.2 ) return W . linear ( x , size ) default_spec = ( ( None , 32 , 1 , 2 , conv_bn_relu ), # t, c, n, s, operator ( 1 , 16 , 1 , 1 , bottleneck ), ( 6 , 24 , 2 , 2 , bottleneck ), ( 6 , 32 , 3 , 2 , bottleneck ), ( 6 , 64 , 4 , 2 , bottleneck ), ( 6 , 96 , 3 , 1 , bottleneck ), ( 6 , 160 , 3 , 2 , bottleneck ), ( 6 , 320 , 1 , 1 , bottleneck ), ( None , 1280 , 1 , 1 , conv1x1 ), ( None , None , 1 , None , pool ), ( None , 1000 , 1 , None , classify ), ) class MobileNetV2 ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , [ 2 , 3 , 224 , 224 ]) def forward ( self , x ): for t , c , n , s , op in default_spec : for i in range ( n ): stride = s if i == 0 else 1 x = op ( x , c , stride , t ) return x net = MobileNetV2 () Torch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 # code based on torchvision/models/mobilenet.py import torch import torch.nn as nn import torch.nn.functional as F class ConvBNReLU ( nn . Sequential ): def __init__ ( self , in_planes , out_planes , kernel_size = 3 , stride = 1 , groups = 1 ): padding = ( kernel_size - 1 ) // 2 super ( ConvBNReLU , self ) . __init__ ( nn . Conv2d ( in_planes , out_planes , kernel_size , stride , padding , groups = groups , bias = False ), nn . BatchNorm2d ( out_planes ), nn . ReLU6 ( inplace = True ), ) class BottleNeck ( nn . Module ): def __init__ ( self , inp , oup , stride , expand_ratio ): super () . __init__ () self . stride = stride assert stride in [ 1 , 2 ] hidden_dim = int ( round ( inp * expand_ratio )) self . use_res_connect = self . stride == 1 and inp == oup layers = [] if expand_ratio != 1 : layers . append ( ConvBNReLU ( inp , hidden_dim , kernel_size = 1 )) layers . extend ([ ConvBNReLU ( hidden_dim , hidden_dim , stride = stride , groups = hidden_dim ), nn . Conv2d ( hidden_dim , oup , 1 , 1 , 0 , bias = False ), nn . BatchNorm2d ( oup ), ]) self . conv = nn . Sequential ( * layers ) def forward ( self , x ): if self . use_res_connect : return x + self . conv ( x ) else : return self . conv ( x ) default_spec = [ [ 1 , 16 , 1 , 1 ], # t, c, n, s [ 6 , 24 , 2 , 2 ], [ 6 , 32 , 3 , 2 ], [ 6 , 64 , 4 , 2 ], [ 6 , 96 , 3 , 1 ], [ 6 , 160 , 3 , 2 ], [ 6 , 320 , 1 , 1 ], ] class MobileNetV2 ( nn . Module ): def __init__ ( self ): super () . __init__ () input_channel = 32 last_channel = 1280 features = [ ConvBNReLU ( 3 , input_channel , stride = 2 )] for t , c , n , s in default_spec : output_channel = c for i in range ( n ): stride = s if i == 0 else 1 features . append ( BottleNeck ( input_channel , output_channel , stride , expand_ratio = t )) input_channel = output_channel features . append ( ConvBNReLU ( input_channel , last_channel , kernel_size = 1 )) self . features = nn . Sequential ( * features ) self . classifier = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( last_channel , 1000 ), ) def forward ( self , x ): x = self . features ( x ) x = x . mean ([ 2 , 3 ]) x = self . classifier ( x ) return x net = MobileNetV2 ()","title":"MobileNet"},{"location":"docs/example/#transformer","text":"\"\"\" The Transformer model from paper Attention is all you need. The Transformer instance accepts two inputs: x is Tensor with shape (Batch, Channel, LengthX). usually a source sequence from embedding (in such cases, Channel equals the embedding size). y is Tensor with shape (Batch, Channel, lengthY). usually a target sequence, also from embedding. **kw is passed down to inner components. \"\"\" import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def multi_head_attention ( x , y = None , num_head = 8 , dropout = 0.1 , mask = None , ** kw ): def split_heads ( t ): return t . reshape ( batch , num_head , size // num_head , t . shape [ - 1 ]) def merge_heads ( t ): return t . reshape ( batch , - 1 , t . shape [ - 1 ]) if y is None : y = x # self attention batch , size = x . shape [: 2 ] assert size % num_head == 0 , 'num_head must be a divisor of size.' assert y . shape [: 2 ] == x . shape [: 2 ], 'The first 2 dims of x, y must match.' q = W . linear ( x , size ) # query k = W . linear ( y , size ) # key v = W . linear ( y , size ) # value q = split_heads ( q ) k = split_heads ( k ) v = split_heads ( v ) q *= ( size // num_head ) ** ( - 0.5 ) a = q . transpose ( 2 , 3 ) . contiguous () . matmul ( k ) # attention weights if mask is not None : a += mask a = F . softmax ( a , dim =- 1 ) a = W . dropout ( a , dropout ) x = v . matmul ( a . transpose ( 2 , 3 ) . contiguous ()) x = merge_heads ( x ) return W . linear ( x , size ) def feed_forward ( x , size_ff = 2048 , dropout = 0.1 , ** kw ): y = W . linear ( x , size_ff , activation = 'relu' ) y = W . dropout ( y , dropout ) return W . linear ( y , x . shape [ 1 ]) def residual_add ( x , layer , dropout = 0.1 , ** kw ): y = W . layer_norm ( x ) y = layer ( y , ** kw ) y = W . dropout ( y , dropout ) return x + y def encoder ( x , num_encoder = 6 , ** kw ): for i in range ( num_encoder ): x = residual_add ( x , multi_head_attention , ** kw ) x = residual_add ( x , feed_forward , ** kw ) return W . layer_norm ( x ) def decoder ( x , y , num_decoder = 6 , mask_x = None , mask_y = None , ** kw ): for i in range ( num_decoder ): y = residual_add ( y , multi_head_attention , mask = mask_y , ** kw ) y = residual_add ( x , multi_head_attention , y = y , mask = mask_x , ** kw ) y = residual_add ( y , feed_forward , ** kw ) return W . layer_norm ( y ) def transformer ( x , y , ** kw ): x = encoder ( x , ** kw ) x = decoder ( x , y , ** kw ) return x class Transformer ( nn . Module ): def __init__ ( self , * shape , ** kw ): super () . __init__ () self . kw = kw warm . up ( self , * shape ) def forward ( self , x , y ): return transformer ( x , y , ** self . kw )","title":"Transformer"},{"location":"docs/example/#efficientnet","text":"For a brief overview, check the blog post . \"\"\" EfficientNet model from https://arxiv.org/abs/1905.11946 \"\"\" import torch import torch.nn as nn import torch.nn.functional as F import warm import warm.functional as W def swish ( x ): return x * torch . sigmoid ( x ) def squeeze_excitation ( x , size_se ): if size_se == 0 : return x size_in = x . shape [ 1 ] x = F . adaptive_avg_pool2d ( x , 1 ) x = W . conv ( x , size_se , 1 , activation = swish ) return W . conv ( x , size_in , 1 , activation = swish ) def drop_connect ( x , rate ): if rate == 0 : return x rate = 1.0 - rate drop_mask = rate + torch . rand ([ x . shape [ 0 ], 1 , 1 , 1 ], device = x . device , requires_grad = False ) return x / rate * drop_mask . floor () def conv_pad_same ( x , size , kernel = 1 , stride = 1 , ** kw ): \"\"\" Same padding so that out_size*stride == in_size. \"\"\" pad = 0 if kernel != 1 or stride != 1 : in_size , s , k = [ torch . as_tensor ( v ) for v in ( x . shape [ 2 :], stride , kernel )] pad = torch . max ((( in_size + s - 1 ) // s - 1 ) * s + k - in_size , torch . tensor ( 0 )) left , right = pad // 2 , pad - pad // 2 if torch . all ( left == right ): pad = tuple ( left . tolist ()) else : left , right = left . tolist (), right . tolist () pad = sum ( zip ( left [:: - 1 ], right [:: - 1 ]), ()) x = F . pad ( x , pad ) pad = 0 return W . conv ( x , size , kernel , stride = stride , padding = pad , ** kw ) def conv_bn_act ( x , size , kernel = 1 , stride = 1 , groups = 1 , bias = False , eps = 1e-3 , momentum = 1e-2 , act = swish ): x = conv_pad_same ( x , size , kernel , stride = stride , groups = groups , bias = bias ) return W . batch_norm ( x , eps = eps , momentum = momentum , activation = act ) def mb_block ( x , size_out , expand = 1 , kernel = 1 , stride = 1 , se_ratio = 0.25 , dc_ratio = 0.2 ): \"\"\" Mobilenet Bottleneck Block. \"\"\" size_in = x . shape [ 1 ] size_mid = size_in * expand y = conv_bn_act ( x , size_mid , 1 ) if expand > 1 else x y = conv_bn_act ( y , size_mid , kernel , stride = stride , groups = size_mid ) y = squeeze_excitation ( y , int ( size_in * se_ratio )) y = conv_bn_act ( y , size_out , 1 , act = None ) if stride == 1 and size_in == size_out : y = drop_connect ( y , dc_ratio ) y += x return y spec_b0 = ( # size, expand, kernel, stride, repeat, squeeze_excitation, drop_connect ( 16 , 1 , 3 , 1 , 1 , 0.25 , 0.2 ), ( 24 , 6 , 3 , 2 , 2 , 0.25 , 0.2 ), ( 40 , 6 , 5 , 2 , 2 , 0.25 , 0.2 ), ( 80 , 6 , 3 , 2 , 3 , 0.25 , 0.2 ), ( 112 , 6 , 5 , 1 , 3 , 0.25 , 0.2 ), ( 192 , 6 , 5 , 2 , 4 , 0.25 , 0.2 ), ( 320 , 6 , 3 , 1 , 1 , 0.25 , 0.2 ), ) class WarmEfficientNet ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , [ 2 , 3 , 32 , 32 ]) def forward ( self , x ): x = conv_bn_act ( x , 32 , kernel = 3 , stride = 2 ) for size , expand , kernel , stride , repeat , se , dc in spec_b0 : for i in range ( repeat ): stride = stride if i == 0 else 1 x = mb_block ( x , size , expand , kernel , stride , se , dc ) x = conv_bn_act ( x , 1280 ) x = F . adaptive_avg_pool2d ( x , 1 ) x = W . dropout ( x , 0.2 ) x = x . view ( x . shape [ 0 ], - 1 ) x = W . linear ( x , 1000 ) return x","title":"EfficientNet"},{"location":"docs/tutorial/","text":"PyWarm Basic Tutorial Import To get started, first import PyWarm in your project: import warm import warm.functional as W Rewrite Now you can replace child module definitions with function calls. For example, instead of: # Torch class MyModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( in_channels , out_channels , kernel_size ) # other child module definitions def forward ( self , x ): x = self . conv1 ( x ) # more forward steps You now use the warm functions: # Warm class MyWarmModule ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , input_shape_or_data ) def forward ( self , x ): x = W . conv ( x , out_channels , kernel_size ) # no in_channels needed # more forward steps Notice the warm.up(self, input_shape_or_data) at the end of the __init__() method. It is required so that PyWarm can infer all shapes of itermediate steps and set up trainable parameters. The only argument input_shape_or_data can either be a tensor, e.g. torch.randn(2, 1, 28, 28) , or just the shape, e.g. [2, 1, 28, 28] for the model inputs. If the model has multiple inputs, you may supple them in a list or a dictionary. Although it is recommended that you attach warm.up() to the end of the __init__() of your model, you can actually use it on the class instances outside of the definition, like a normal function call: class MyWarmModule ( nn . Module ): def __init__ ( self ): super () . __init__ () # no warm.up here def forward ( self , x ): x = W . conv ( x , 10 , 3 ) # forward step, powered by PyWarm model = MyWarmModule () # call warm.up outside of the module definition warm . up ( model , [ 2 , 1 , 28 , 28 ]) Note : If the model contains batch_norm layers, you need to specify the Batch dimension to at least 2. Advanced Topics Default shapes PyWarm has a unified functional interface, that by default all functions accept and return tensors with shape (Batch, Channel, *) , where * is any number of additional dimensions. For example, for 2d images, the * usually stands for (Height, Width) , and for 1d time series, the * means (Time,) . This convention is optimized for the performance of Convolutional networks. It may become less efficient if your model relies heavily on dense (Linear) or recurrent (LSTM, GRU) layers. You can use different input and output shapes by specifying in_shape , out_shape keyword arguments in the function calls. These keywords accept only letters 'B' , 'C' and 'D' , which stand for Batch , Channel , and * (extra Dimensions) respectively. So for example if for a 1d time series you want to have (Time, Batch, Channel) as the output shape, you can specify out_shape='DBC' . Dimensional awareness PyWarm functions can automatically identify 1d, 2d and 3d input data, so the same function can be used on different dimensional cases. For example, the single W.conv is enough to replace nn.Conv1d, nn.Conv2d, nn.Conv3d . Similarly, you don't need nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d for differnt inputs, a single W.batch_norm can replace them all. Shape inference Many neural network layers will perform a transformation of shapes. For example, after a convolution operation, the shape is changed from (Batch, ChannelIn, *) to (Batch, ChannelOut, *) . PyTorch nn Modules require the user to keep track of both in_channels and out_channels . PyWarm relieves this pain by inferring the in_channels for you, so you can focus more on the nature of your tasks, rather than chores. Argument passdown If the signature of a PyWarm function does not specify all possible argument of its torch nn Module couterpart, it will pass down additional keyword arguments to the underlying nn Module. For example, if you want to specify strides of 2 for a conv layer, just use W.conv(..., stride=2) . The only thing to remember is that you have to specify the full keyword, instead of relying on the position of arguments. Parameter initialization per layer Unlike PyTorch's approach, paramter initialization can be specified directly in PyWarm's functional interface. For example: x = W . conv ( x , 20 , 1 , init_weight = 'kaiming_uniform_' ) This makes it easier to create layer specific initialization in PyWarm. You no long need to go through self.modules() and self.parameters() to create custom initializations. By default, PyWarm will look into torch.nn.init for initialization function names. Alternatively, you may just specify a callable, or a tuple (fn, kwargs) if the callable accepts more than 1 input. If the initialization is not specified or None is used, the corresponding layer will get default initializations as used in torch nn modules. Apply activation nonlinearity to the output PyWarm's functional interface supports adding an optional keyword argument activation=name , where name is a callable or just its name, which represents an activation (nonlinearity) function in torch.nn.functional or just torch . By default no activation is used. Mix and Match You are not limited to only use PyWarm's functional interface. It is completely ok to mix and match the old PyTorch way of child module definitions with PyWarm's function API. For example: class MyModel ( nn . Module ): def __init__ ( self ): super () . __init__ () # other stuff self . conv1 = nn . Conv2d ( 2 , 30 , 7 , padding = 3 ) # other stuff def forward ( self , x ): y = F . relu ( self . conv1 ( x )) y = W . conv ( y , 40 , 3 , activation = 'relu' ) Custom layer names Normally you do not have to specify layer names when using the functional API. PyWarm will track and count usage for the layer type and automatically assign names for you. For example, subsequent convolutional layer calls via W.conv will create conv_1 , conv_2 , ... etc. in the parent module. Nevertheless, if you want to ensure certain layer have particular names, you can specify name='my_name' keyword arguments in the call. Alternatively, if you still want PyWarm to count usage and increment ordinal for you, but only want to customize the base type name, you can use base_name='my_prefix' keyword instead. The PyWarm modules will then have names like my_prefix_1 , my_prefix_2 in the parent module. See the PyWarm resnet example in the examples folder on how to use these features to load pre-trained model parameters into PyWarm models.","title":"Tutorial"},{"location":"docs/tutorial/#pywarm-basic-tutorial","text":"","title":"PyWarm Basic Tutorial"},{"location":"docs/tutorial/#import","text":"To get started, first import PyWarm in your project: import warm import warm.functional as W","title":"Import"},{"location":"docs/tutorial/#rewrite","text":"Now you can replace child module definitions with function calls. For example, instead of: # Torch class MyModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( in_channels , out_channels , kernel_size ) # other child module definitions def forward ( self , x ): x = self . conv1 ( x ) # more forward steps You now use the warm functions: # Warm class MyWarmModule ( nn . Module ): def __init__ ( self ): super () . __init__ () warm . up ( self , input_shape_or_data ) def forward ( self , x ): x = W . conv ( x , out_channels , kernel_size ) # no in_channels needed # more forward steps Notice the warm.up(self, input_shape_or_data) at the end of the __init__() method. It is required so that PyWarm can infer all shapes of itermediate steps and set up trainable parameters. The only argument input_shape_or_data can either be a tensor, e.g. torch.randn(2, 1, 28, 28) , or just the shape, e.g. [2, 1, 28, 28] for the model inputs. If the model has multiple inputs, you may supple them in a list or a dictionary. Although it is recommended that you attach warm.up() to the end of the __init__() of your model, you can actually use it on the class instances outside of the definition, like a normal function call: class MyWarmModule ( nn . Module ): def __init__ ( self ): super () . __init__ () # no warm.up here def forward ( self , x ): x = W . conv ( x , 10 , 3 ) # forward step, powered by PyWarm model = MyWarmModule () # call warm.up outside of the module definition warm . up ( model , [ 2 , 1 , 28 , 28 ]) Note : If the model contains batch_norm layers, you need to specify the Batch dimension to at least 2.","title":"Rewrite"},{"location":"docs/tutorial/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"docs/tutorial/#default-shapes","text":"PyWarm has a unified functional interface, that by default all functions accept and return tensors with shape (Batch, Channel, *) , where * is any number of additional dimensions. For example, for 2d images, the * usually stands for (Height, Width) , and for 1d time series, the * means (Time,) . This convention is optimized for the performance of Convolutional networks. It may become less efficient if your model relies heavily on dense (Linear) or recurrent (LSTM, GRU) layers. You can use different input and output shapes by specifying in_shape , out_shape keyword arguments in the function calls. These keywords accept only letters 'B' , 'C' and 'D' , which stand for Batch , Channel , and * (extra Dimensions) respectively. So for example if for a 1d time series you want to have (Time, Batch, Channel) as the output shape, you can specify out_shape='DBC' .","title":"Default shapes"},{"location":"docs/tutorial/#dimensional-awareness","text":"PyWarm functions can automatically identify 1d, 2d and 3d input data, so the same function can be used on different dimensional cases. For example, the single W.conv is enough to replace nn.Conv1d, nn.Conv2d, nn.Conv3d . Similarly, you don't need nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d for differnt inputs, a single W.batch_norm can replace them all.","title":"Dimensional awareness"},{"location":"docs/tutorial/#shape-inference","text":"Many neural network layers will perform a transformation of shapes. For example, after a convolution operation, the shape is changed from (Batch, ChannelIn, *) to (Batch, ChannelOut, *) . PyTorch nn Modules require the user to keep track of both in_channels and out_channels . PyWarm relieves this pain by inferring the in_channels for you, so you can focus more on the nature of your tasks, rather than chores.","title":"Shape inference"},{"location":"docs/tutorial/#argument-passdown","text":"If the signature of a PyWarm function does not specify all possible argument of its torch nn Module couterpart, it will pass down additional keyword arguments to the underlying nn Module. For example, if you want to specify strides of 2 for a conv layer, just use W.conv(..., stride=2) . The only thing to remember is that you have to specify the full keyword, instead of relying on the position of arguments.","title":"Argument passdown"},{"location":"docs/tutorial/#parameter-initialization-per-layer","text":"Unlike PyTorch's approach, paramter initialization can be specified directly in PyWarm's functional interface. For example: x = W . conv ( x , 20 , 1 , init_weight = 'kaiming_uniform_' ) This makes it easier to create layer specific initialization in PyWarm. You no long need to go through self.modules() and self.parameters() to create custom initializations. By default, PyWarm will look into torch.nn.init for initialization function names. Alternatively, you may just specify a callable, or a tuple (fn, kwargs) if the callable accepts more than 1 input. If the initialization is not specified or None is used, the corresponding layer will get default initializations as used in torch nn modules.","title":"Parameter initialization per layer"},{"location":"docs/tutorial/#apply-activation-nonlinearity-to-the-output","text":"PyWarm's functional interface supports adding an optional keyword argument activation=name , where name is a callable or just its name, which represents an activation (nonlinearity) function in torch.nn.functional or just torch . By default no activation is used.","title":"Apply activation nonlinearity to the output"},{"location":"docs/tutorial/#mix-and-match","text":"You are not limited to only use PyWarm's functional interface. It is completely ok to mix and match the old PyTorch way of child module definitions with PyWarm's function API. For example: class MyModel ( nn . Module ): def __init__ ( self ): super () . __init__ () # other stuff self . conv1 = nn . Conv2d ( 2 , 30 , 7 , padding = 3 ) # other stuff def forward ( self , x ): y = F . relu ( self . conv1 ( x )) y = W . conv ( y , 40 , 3 , activation = 'relu' )","title":"Mix and Match"},{"location":"docs/tutorial/#custom-layer-names","text":"Normally you do not have to specify layer names when using the functional API. PyWarm will track and count usage for the layer type and automatically assign names for you. For example, subsequent convolutional layer calls via W.conv will create conv_1 , conv_2 , ... etc. in the parent module. Nevertheless, if you want to ensure certain layer have particular names, you can specify name='my_name' keyword arguments in the call. Alternatively, if you still want PyWarm to count usage and increment ordinal for you, but only want to customize the base type name, you can use base_name='my_prefix' keyword instead. The PyWarm modules will then have names like my_prefix_1 , my_prefix_2 in the parent module. See the PyWarm resnet example in the examples folder on how to use these features to load pre-trained model parameters into PyWarm models.","title":"Custom layer names"},{"location":"reference/warm/","text":"Module warm warm.up is an alias of warm.engine.prepare_model_ . Sub-modules warm.engine warm.functional warm.module warm.util","title":"Index"},{"location":"reference/warm/#module-warm","text":"warm.up is an alias of warm.engine.prepare_model_ .","title":"Module warm"},{"location":"reference/warm/#sub-modules","text":"warm.engine warm.functional warm.module warm.util","title":"Sub-modules"},{"location":"reference/warm/engine/","text":"Module warm.engine PyWarm engine to the functional interface. Functions activate def : x , spec , lookup = None Activate tensors with given nonlinearity spec ification. x: Tensor or list of Tensor ; The tensors to be initialized. spec: str or callable or 2-tuple ; If a str , should be one of the nonlinearity functions contained in torch.nn.functional or torch . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . lookup: None or list of module ; Parent modules to look for spec . If None , [nn.functional, torch] is used. return: Tensor or list of Tensor ; Activation results. forward def : x , base_class , base_name = None , name = None , base_arg = None , base_kw = None , parent = None , infer_kw = None , in_shape = 'BCD' , base_shape = 'BCD' , out_shape = 'BCD' , tuple_out = False , forward_arg = None , forward_kw = None , initialization = None , activation = None , ** kw A forward template that creates child instances at the first time it is called. x: Tensor ; The nd-tensor to be forwarded. base_class: Module ; A child torch.nn.Module that will be created at the first time this function is called. base_name: str ; Name for the base_class . Default: base_class name. name: str ; Name for the child module instance. Default: class name plus ordinal. base_arg: tuple ; Positional args to be passed to create the child module instance. Default: None. base_kw: dict ; KWargs to be passed to create the child module instance. Default: None. parent: Module ; The parent of the child instance. Default: None. If None , will use get_default_parent . infer_kw: dict ; Key should be valid for the child instance. Value shoud be a character, one of 'B' , 'C' , or 'D' (see permute ), to substitute for a dimension of x . Default: None. in_shape: str ; The dimension shape of x . See also permute . Default: 'BCD' . base_shape: str ; The dimension shape required by the child module. See also permute . Default: 'BCD' . out_shape: str or tuple or None ; The dimension shape of returned tensor. See also permute . Default: 'BCD' . tuple_out: bool ; Whether the child module will return more than 1 outputs (e.g. nn.RNN ). If True , the returned value of the function will be a tuple containing all outputs. Default: False. forward_arg: tuple ; positional args to be passed when calling the child module instance. Default: None. forward_kw: dict ; KWargs to be passed when calling the child module instance. Default: None. initialization: dict ; Keys are name of parameters to initialize. Values are init specs, which can be a, str , a callable , or 2-tuple ; See the spec argument of initialize_ for details. Default: None. activation: str or callable or 2-tuple ; See the spec argument of activate . Default: None. return: Tensor or tuple ; If tuple_out is True , the returned value will be a tuple . get_default_parent def : Get the default parent module. initialize_ def : x , spec Initialize parameters with given nonlinearity spec ification. x: Tensor or list of Tensor ; The tensors to be initialized. spec: str or callable or 2-tuple ; If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . is_ready def : model Check if a model is prepared. namespace def : f After decoration, the function name and call count will be appended to the name kw. permute def : x , in_shape = 'BCD' , out_shape = 'BCD' , ** kw Permute the dimensions of a tensor. x: Tensor ; The nd-tensor to be permuted. in_shape: str ; The dimension shape of x . Can only have characters 'B' or 'C' or 'D' , which stand for Batch, Channel, or extra Dimensions. The default value 'BCD' means the input tensor x should be at lest 2-d with shape (Batch, Channel, Dim0, Dim1, Dim2, ...) , where Dim0, Dim1, Dim2 ... stand for any number of extra dimensions. out_shape: str or tuple or None ; The dimension shape of returned tensor. Default: 'BCD' . If a str , it is restricted to the same three characters 'B' , 'C' or 'D' as the in_shape . If a tuple , in_shape is ignored, and simply x.permute(out_shape) is returned. If None , no permution will be performed. return: Tensor ; Permuted nd-tensor. prepare_model_ def : model , * data , device = 'cpu' Initialize all childen modules defined by warm in a parent model . model: Module ; The parent model to be prepared. data: Tensor, or list of int ; A batch of data with the correct shape and type to be forwarded by model. data can also be a list of int , in which case it is interpreted as the shape of the input data. device: str, or torch.device ; Should be the same for model and data . Default: 'cpu' . return: Module ; The prepared model, with all children modules defined by warm initialized. set_default_parent def : parent Set the default parent module. unused_kwargs def : kw Filter out entries used by forward and return the rest.","title":"Engine"},{"location":"reference/warm/engine/#module-warmengine","text":"PyWarm engine to the functional interface.","title":"Module warm.engine"},{"location":"reference/warm/engine/#functions","text":"","title":"Functions"},{"location":"reference/warm/engine/#activate","text":"def : x , spec , lookup = None Activate tensors with given nonlinearity spec ification. x: Tensor or list of Tensor ; The tensors to be initialized. spec: str or callable or 2-tuple ; If a str , should be one of the nonlinearity functions contained in torch.nn.functional or torch . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . lookup: None or list of module ; Parent modules to look for spec . If None , [nn.functional, torch] is used. return: Tensor or list of Tensor ; Activation results.","title":"activate"},{"location":"reference/warm/engine/#forward","text":"def : x , base_class , base_name = None , name = None , base_arg = None , base_kw = None , parent = None , infer_kw = None , in_shape = 'BCD' , base_shape = 'BCD' , out_shape = 'BCD' , tuple_out = False , forward_arg = None , forward_kw = None , initialization = None , activation = None , ** kw A forward template that creates child instances at the first time it is called. x: Tensor ; The nd-tensor to be forwarded. base_class: Module ; A child torch.nn.Module that will be created at the first time this function is called. base_name: str ; Name for the base_class . Default: base_class name. name: str ; Name for the child module instance. Default: class name plus ordinal. base_arg: tuple ; Positional args to be passed to create the child module instance. Default: None. base_kw: dict ; KWargs to be passed to create the child module instance. Default: None. parent: Module ; The parent of the child instance. Default: None. If None , will use get_default_parent . infer_kw: dict ; Key should be valid for the child instance. Value shoud be a character, one of 'B' , 'C' , or 'D' (see permute ), to substitute for a dimension of x . Default: None. in_shape: str ; The dimension shape of x . See also permute . Default: 'BCD' . base_shape: str ; The dimension shape required by the child module. See also permute . Default: 'BCD' . out_shape: str or tuple or None ; The dimension shape of returned tensor. See also permute . Default: 'BCD' . tuple_out: bool ; Whether the child module will return more than 1 outputs (e.g. nn.RNN ). If True , the returned value of the function will be a tuple containing all outputs. Default: False. forward_arg: tuple ; positional args to be passed when calling the child module instance. Default: None. forward_kw: dict ; KWargs to be passed when calling the child module instance. Default: None. initialization: dict ; Keys are name of parameters to initialize. Values are init specs, which can be a, str , a callable , or 2-tuple ; See the spec argument of initialize_ for details. Default: None. activation: str or callable or 2-tuple ; See the spec argument of activate . Default: None. return: Tensor or tuple ; If tuple_out is True , the returned value will be a tuple .","title":"forward"},{"location":"reference/warm/engine/#get_default_parent","text":"def : Get the default parent module.","title":"get_default_parent"},{"location":"reference/warm/engine/#initialize_","text":"def : x , spec Initialize parameters with given nonlinearity spec ification. x: Tensor or list of Tensor ; The tensors to be initialized. spec: str or callable or 2-tuple ; If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) .","title":"initialize_"},{"location":"reference/warm/engine/#is_ready","text":"def : model Check if a model is prepared.","title":"is_ready"},{"location":"reference/warm/engine/#namespace","text":"def : f After decoration, the function name and call count will be appended to the name kw.","title":"namespace"},{"location":"reference/warm/engine/#permute","text":"def : x , in_shape = 'BCD' , out_shape = 'BCD' , ** kw Permute the dimensions of a tensor. x: Tensor ; The nd-tensor to be permuted. in_shape: str ; The dimension shape of x . Can only have characters 'B' or 'C' or 'D' , which stand for Batch, Channel, or extra Dimensions. The default value 'BCD' means the input tensor x should be at lest 2-d with shape (Batch, Channel, Dim0, Dim1, Dim2, ...) , where Dim0, Dim1, Dim2 ... stand for any number of extra dimensions. out_shape: str or tuple or None ; The dimension shape of returned tensor. Default: 'BCD' . If a str , it is restricted to the same three characters 'B' , 'C' or 'D' as the in_shape . If a tuple , in_shape is ignored, and simply x.permute(out_shape) is returned. If None , no permution will be performed. return: Tensor ; Permuted nd-tensor.","title":"permute"},{"location":"reference/warm/engine/#prepare_model_","text":"def : model , * data , device = 'cpu' Initialize all childen modules defined by warm in a parent model . model: Module ; The parent model to be prepared. data: Tensor, or list of int ; A batch of data with the correct shape and type to be forwarded by model. data can also be a list of int , in which case it is interpreted as the shape of the input data. device: str, or torch.device ; Should be the same for model and data . Default: 'cpu' . return: Module ; The prepared model, with all children modules defined by warm initialized.","title":"prepare_model_"},{"location":"reference/warm/engine/#set_default_parent","text":"def : parent Set the default parent module.","title":"set_default_parent"},{"location":"reference/warm/engine/#unused_kwargs","text":"def : kw Filter out entries used by forward and return the rest.","title":"unused_kwargs"},{"location":"reference/warm/functional/","text":"Module warm.functional Wraps around various torch.nn Modules to fit into a functional interface. Functions batch_norm def : x , ** kw Batch Normalization layer. x: Tensor ; 2d or more, with shapes (Batch, Channel, *) where * means any number of additional dimensions. **kw: dict ; Any additional KWargs are passed down to torch.nn.BatchNormNd , where N can be 1, 2 or 3. as well as warm.engine.forward . Refer to their docs for details. Some of the additional BatchNorm arguments: eps, momentum, affine, track_running_stats . return: Tensor ; Same shape as input x . conv def : x , size , kernel , init_weight = None , init_bias = None , bias = True , ** kw Convolution layer. x: Tensor ; With shape (Batch, Channel, *) where * Can be 1d or 2d or 3d. If 3d, shapes are (Batch, Channel, Length) . If 4d, shapes are (Batch, Channel, Height, Width) . If 5d, shapes are (Batch, Channel, Depth, Height, Width) . size: int ; Size of hidden filters, and size of the output channel. kernel: int or tuple ; Size of the convolution kernel. init_weight: None or str or callable ; Initialization specification for the weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: None , and the weight tensor is initialized using torch.nn.ConvNd s default scheme. init_bias: None or str or callable ; Same as init_weight , but for the bias tensor. bias: bool ; If True , adds a learnable bias to the output. Default: True . **kw:dict ; Any additional KWargs are passed down to torch.nn.ConvNd , where N can be 1, 2 or 3. as well as warm.engine.forward . Refer to their docs for details. Some of the additional ConvNd arguments: stride, padding, dilation, groups . return: Tensor ; With shape (Batch, Size, *) where * can be 1d, 2d, 3d that depends on x . dropout def : x , rate = 0.5 , by_channel = False , ** kw Dropout layer. During training, randomly zeros part of input tensor x , at probability rate . x: Tensor ; Can be of any shape if by_channel is false, or 2d and up if by_channel is true. rate: float ; The probability of dropout. Default 0.5. by_channel: bool ; If true, will dropout entire channels (all 'D' dimensions will be 0 if x is 'BCD' ). by_channel true requires x to be 2d or more. inplace: bool ; If true, the operation will be in-place and the input x will be altered. return: Tensor ; Same shape as x . embedding def : x , size , vocabulary = None , ** kw Embedding layer. The input is usually a list of indices (integers), and the output is a dense matrix which maps indices to dense vectors. Thus the output will have 1 more dimension than the input. Note : The output of this function is always one more dimension than the input. For input with shape (*) , The output will be (*, size) . Any shape specifications in the KWargs are ignored. x: Tensor ; Contains indices into the vocabulary. Will be converted to LongTensor of integers. Can be of any shape. size: int ; The size of embedding vector. vocabulary: int or None ; The size of vocabulary of embedding, or max number of unique indices in x . By default it is set to max(x)-min(x)+1 . **kw: dict ; Any additional KWargs are passed down to torch.nn.LayerNorm , as well as warm.engine.forward . return: Tensor ; With the embedded dim appended to the shape of x. Thus with shape (*, Size) , where * is the shape of x . gru def : * arg , ** kw Gated Recurrent Unit layer. x: Tensor or tuple ; If tuple, must be of format (x, (h_0, c_0)) , where x is a 3d tensor, with shapes (Batch, Channel, Length) . size: int ; Size of hidden features, and size of the output channel. init_weight_hh: None or str or callable ; Initialization specification for the hidden-hidden weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: 'orthogonal_' . init_weight_ih: None or str or callable ; Initialization specification for the input-hidden weight tensor. Default: None , and the weight tensor is initialized using torch.nn.GRU s default scheme. init_bias_hh: None or str or callable ; Initialization specification for the hidden-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.GRU s default scheme. init_bias_ih: None or str or callable ; Initialization specification for the input-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.GRU s default scheme. bias: bool ; If False , then the layer does not use bias_ih and bias_hh . Default: True . num_layers: int ; Number of the recurrent layers. Default: 1. tuple_out: bool ; If True , the returned value will be a tuple (out, (h_n, c_n)) . Default: False. **kw: dict ; Any additional KWargs are passed down to torch.nn.GRU , as well as warm.engine.forward . Refer to their docs for details. Some of the additional GRU arguments: dropout, bidirectional, batch_first . return: Tensor or tuple ; If tuple_out set to true, will return (out, (h_n, c_n) , otherwise just out . out has shape (Batch, Size, Length*Directions) , where Directions = 2 if bidirectional else 1. h_n is the hidden states with shape (num_layers*Directions, Batch, Size) . c_n is the cell states with shape (num_layers*Directions, Batch, Size) . identity def : x , * arg , ** kw Identity layer that returns the first input, ignores the rest arguments. layer_norm def : x , dim = 1 , ** kw Layer Normalization. x: Tensor ; Can be of any shape. dim: int or list of int ; Dimensions to be normalized. Default: 1. **kw: dict ; Any additional KWargs are passed down to torch.nn.LayerNorm , as well as warm.engine.forward . return: Tensor ; Same shape as x . linear def : x , size , init_weight = None , init_bias = None , bias = True , ** kw Linear transformation layer. x: Tensor ; 2d or more, with shapes (Batch, Channel, *) where * means any number of additional dimensions. size: int ; Size of hidden features, and size of the output channel. init_weight: None or str or callable ; Initialization specification for the weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: None , and the weight tensor is initialized using torch.nn.Linear s default scheme. init_bias: None or str or callable ; Same as init_weight , but for the bias tensor. bias: bool ; If True , adds a learnable bias to the output. Default: True . **kw:dict ; Any additional KWargs are passed down to warm.engine.forward . Refer to its docs for details. return: Tensor ; With shape (Batch, Size, *) where * can be 1d, 2d, 3d that depends on x . lstm def : x , size , init_weight_hh = 'orthogonal_' , init_weight_ih = None , init_bias_hh = None , init_bias_ih = None , bias = True , num_layers = 1 , ** kw Long Short Term Memory layer. x: Tensor or tuple ; If tuple, must be of format (x, (h_0, c_0)) , where x is a 3d tensor, with shapes (Batch, Channel, Length) . size: int ; Size of hidden features, and size of the output channel. init_weight_hh: None or str or callable ; Initialization specification for the hidden-hidden weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: 'orthogonal_' . init_weight_ih: None or str or callable ; Initialization specification for the input-hidden weight tensor. Default: None , and the weight tensor is initialized using torch.nn.LSTM s default scheme. init_bias_hh: None or str or callable ; Initialization specification for the hidden-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.LSTM s default scheme. init_bias_ih: None or str or callable ; Initialization specification for the input-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.LSTM s default scheme. bias: bool ; If False , then the layer does not use bias_ih and bias_hh . Default: True . num_layers: int ; Number of the recurrent layers. Default: 1. tuple_out: bool ; If True , the returned value will be a tuple (out, (h_n, c_n)) . Default: False. **kw: dict ; Any additional KWargs are passed down to torch.nn.LSTM , as well as warm.engine.forward . Refer to their docs for details. Some of the additional LSTM arguments: dropout, bidirectional, batch_first . return: Tensor or tuple ; If tuple_out set to true, will return (out, (h_n, c_n) , otherwise just out . out has shape (Batch, Size, Length*Directions) , where Directions = 2 if bidirectional else 1. h_n is the hidden states with shape (num_layers*Directions, Batch, Size) . c_n is the cell states with shape (num_layers*Directions, Batch, Size) . transformer def : x , y = None , num_encoder = 6 , num_decoder = 6 , num_head = 8 , mask = None , causal = False , in_shape = 'BCD' , ** kw Transformer layer. This layer covers functionality of Transformer , TransformerEncoder , and TransformerDecoder . See torch.nn.Transformer for more details. x: Tensor ; The source sequence, with shape (Batch, Channel, LengthX) . Channel is usually from embedding. y: None or Tensor ; The target sequence. Also with shape (Batch, Channel, LengthY) . If not present, default to equal x . num_encoder: int ; Number of encoder layers. Set to 0 to disable encoder and use only decoder. Default 6. num_decoder: int ; Number of decoder layers. Set to 0 to disable decoder and use only encoder. Default 6. num_head: int ; Number of heads for multi-headed attention. Default 8. mask: None or dict ; Keys are among: src_mask , tgt_mask , memory_mask , src_key_padding_mask , tgt_key_padding_mask , memory_key_padding_mask . See the forward method of torch.nn.Transformer for details. causal: bool ; Default false. if true, will add causal masks to source and target, so that current value only depends on the past, not the future, in the sequences. **kw: dict ; Any additional KWargs are passed down to torch.nn.Transformer , as well as warm.engine.forward . return: Tensor ; Same shape as y , if num_decoder > 0. Otherwise same shape as x .","title":"Functional"},{"location":"reference/warm/functional/#module-warmfunctional","text":"Wraps around various torch.nn Modules to fit into a functional interface.","title":"Module warm.functional"},{"location":"reference/warm/functional/#functions","text":"","title":"Functions"},{"location":"reference/warm/functional/#batch_norm","text":"def : x , ** kw Batch Normalization layer. x: Tensor ; 2d or more, with shapes (Batch, Channel, *) where * means any number of additional dimensions. **kw: dict ; Any additional KWargs are passed down to torch.nn.BatchNormNd , where N can be 1, 2 or 3. as well as warm.engine.forward . Refer to their docs for details. Some of the additional BatchNorm arguments: eps, momentum, affine, track_running_stats . return: Tensor ; Same shape as input x .","title":"batch_norm"},{"location":"reference/warm/functional/#conv","text":"def : x , size , kernel , init_weight = None , init_bias = None , bias = True , ** kw Convolution layer. x: Tensor ; With shape (Batch, Channel, *) where * Can be 1d or 2d or 3d. If 3d, shapes are (Batch, Channel, Length) . If 4d, shapes are (Batch, Channel, Height, Width) . If 5d, shapes are (Batch, Channel, Depth, Height, Width) . size: int ; Size of hidden filters, and size of the output channel. kernel: int or tuple ; Size of the convolution kernel. init_weight: None or str or callable ; Initialization specification for the weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: None , and the weight tensor is initialized using torch.nn.ConvNd s default scheme. init_bias: None or str or callable ; Same as init_weight , but for the bias tensor. bias: bool ; If True , adds a learnable bias to the output. Default: True . **kw:dict ; Any additional KWargs are passed down to torch.nn.ConvNd , where N can be 1, 2 or 3. as well as warm.engine.forward . Refer to their docs for details. Some of the additional ConvNd arguments: stride, padding, dilation, groups . return: Tensor ; With shape (Batch, Size, *) where * can be 1d, 2d, 3d that depends on x .","title":"conv"},{"location":"reference/warm/functional/#dropout","text":"def : x , rate = 0.5 , by_channel = False , ** kw Dropout layer. During training, randomly zeros part of input tensor x , at probability rate . x: Tensor ; Can be of any shape if by_channel is false, or 2d and up if by_channel is true. rate: float ; The probability of dropout. Default 0.5. by_channel: bool ; If true, will dropout entire channels (all 'D' dimensions will be 0 if x is 'BCD' ). by_channel true requires x to be 2d or more. inplace: bool ; If true, the operation will be in-place and the input x will be altered. return: Tensor ; Same shape as x .","title":"dropout"},{"location":"reference/warm/functional/#embedding","text":"def : x , size , vocabulary = None , ** kw Embedding layer. The input is usually a list of indices (integers), and the output is a dense matrix which maps indices to dense vectors. Thus the output will have 1 more dimension than the input. Note : The output of this function is always one more dimension than the input. For input with shape (*) , The output will be (*, size) . Any shape specifications in the KWargs are ignored. x: Tensor ; Contains indices into the vocabulary. Will be converted to LongTensor of integers. Can be of any shape. size: int ; The size of embedding vector. vocabulary: int or None ; The size of vocabulary of embedding, or max number of unique indices in x . By default it is set to max(x)-min(x)+1 . **kw: dict ; Any additional KWargs are passed down to torch.nn.LayerNorm , as well as warm.engine.forward . return: Tensor ; With the embedded dim appended to the shape of x. Thus with shape (*, Size) , where * is the shape of x .","title":"embedding"},{"location":"reference/warm/functional/#gru","text":"def : * arg , ** kw Gated Recurrent Unit layer. x: Tensor or tuple ; If tuple, must be of format (x, (h_0, c_0)) , where x is a 3d tensor, with shapes (Batch, Channel, Length) . size: int ; Size of hidden features, and size of the output channel. init_weight_hh: None or str or callable ; Initialization specification for the hidden-hidden weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: 'orthogonal_' . init_weight_ih: None or str or callable ; Initialization specification for the input-hidden weight tensor. Default: None , and the weight tensor is initialized using torch.nn.GRU s default scheme. init_bias_hh: None or str or callable ; Initialization specification for the hidden-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.GRU s default scheme. init_bias_ih: None or str or callable ; Initialization specification for the input-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.GRU s default scheme. bias: bool ; If False , then the layer does not use bias_ih and bias_hh . Default: True . num_layers: int ; Number of the recurrent layers. Default: 1. tuple_out: bool ; If True , the returned value will be a tuple (out, (h_n, c_n)) . Default: False. **kw: dict ; Any additional KWargs are passed down to torch.nn.GRU , as well as warm.engine.forward . Refer to their docs for details. Some of the additional GRU arguments: dropout, bidirectional, batch_first . return: Tensor or tuple ; If tuple_out set to true, will return (out, (h_n, c_n) , otherwise just out . out has shape (Batch, Size, Length*Directions) , where Directions = 2 if bidirectional else 1. h_n is the hidden states with shape (num_layers*Directions, Batch, Size) . c_n is the cell states with shape (num_layers*Directions, Batch, Size) .","title":"gru"},{"location":"reference/warm/functional/#identity","text":"def : x , * arg , ** kw Identity layer that returns the first input, ignores the rest arguments.","title":"identity"},{"location":"reference/warm/functional/#layer_norm","text":"def : x , dim = 1 , ** kw Layer Normalization. x: Tensor ; Can be of any shape. dim: int or list of int ; Dimensions to be normalized. Default: 1. **kw: dict ; Any additional KWargs are passed down to torch.nn.LayerNorm , as well as warm.engine.forward . return: Tensor ; Same shape as x .","title":"layer_norm"},{"location":"reference/warm/functional/#linear","text":"def : x , size , init_weight = None , init_bias = None , bias = True , ** kw Linear transformation layer. x: Tensor ; 2d or more, with shapes (Batch, Channel, *) where * means any number of additional dimensions. size: int ; Size of hidden features, and size of the output channel. init_weight: None or str or callable ; Initialization specification for the weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: None , and the weight tensor is initialized using torch.nn.Linear s default scheme. init_bias: None or str or callable ; Same as init_weight , but for the bias tensor. bias: bool ; If True , adds a learnable bias to the output. Default: True . **kw:dict ; Any additional KWargs are passed down to warm.engine.forward . Refer to its docs for details. return: Tensor ; With shape (Batch, Size, *) where * can be 1d, 2d, 3d that depends on x .","title":"linear"},{"location":"reference/warm/functional/#lstm","text":"def : x , size , init_weight_hh = 'orthogonal_' , init_weight_ih = None , init_bias_hh = None , init_bias_ih = None , bias = True , num_layers = 1 , ** kw Long Short Term Memory layer. x: Tensor or tuple ; If tuple, must be of format (x, (h_0, c_0)) , where x is a 3d tensor, with shapes (Batch, Channel, Length) . size: int ; Size of hidden features, and size of the output channel. init_weight_hh: None or str or callable ; Initialization specification for the hidden-hidden weight tensor. If a str , should be one of the nonlinearity functions contained in torch.nn.init . If a callable , it will be applied to x directly, i.e. spec(x) . If a 2- tuple , it must be of format (callable, kwargs) , i.e. callable(x, **kwargs) . Default: 'orthogonal_' . init_weight_ih: None or str or callable ; Initialization specification for the input-hidden weight tensor. Default: None , and the weight tensor is initialized using torch.nn.LSTM s default scheme. init_bias_hh: None or str or callable ; Initialization specification for the hidden-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.LSTM s default scheme. init_bias_ih: None or str or callable ; Initialization specification for the input-hidden bias tensor. Default: None , and the weight tensor is initialized using torch.nn.LSTM s default scheme. bias: bool ; If False , then the layer does not use bias_ih and bias_hh . Default: True . num_layers: int ; Number of the recurrent layers. Default: 1. tuple_out: bool ; If True , the returned value will be a tuple (out, (h_n, c_n)) . Default: False. **kw: dict ; Any additional KWargs are passed down to torch.nn.LSTM , as well as warm.engine.forward . Refer to their docs for details. Some of the additional LSTM arguments: dropout, bidirectional, batch_first . return: Tensor or tuple ; If tuple_out set to true, will return (out, (h_n, c_n) , otherwise just out . out has shape (Batch, Size, Length*Directions) , where Directions = 2 if bidirectional else 1. h_n is the hidden states with shape (num_layers*Directions, Batch, Size) . c_n is the cell states with shape (num_layers*Directions, Batch, Size) .","title":"lstm"},{"location":"reference/warm/functional/#transformer","text":"def : x , y = None , num_encoder = 6 , num_decoder = 6 , num_head = 8 , mask = None , causal = False , in_shape = 'BCD' , ** kw Transformer layer. This layer covers functionality of Transformer , TransformerEncoder , and TransformerDecoder . See torch.nn.Transformer for more details. x: Tensor ; The source sequence, with shape (Batch, Channel, LengthX) . Channel is usually from embedding. y: None or Tensor ; The target sequence. Also with shape (Batch, Channel, LengthY) . If not present, default to equal x . num_encoder: int ; Number of encoder layers. Set to 0 to disable encoder and use only decoder. Default 6. num_decoder: int ; Number of decoder layers. Set to 0 to disable decoder and use only encoder. Default 6. num_head: int ; Number of heads for multi-headed attention. Default 8. mask: None or dict ; Keys are among: src_mask , tgt_mask , memory_mask , src_key_padding_mask , tgt_key_padding_mask , memory_key_padding_mask . See the forward method of torch.nn.Transformer for details. causal: bool ; Default false. if true, will add causal masks to source and target, so that current value only depends on the past, not the future, in the sequences. **kw: dict ; Any additional KWargs are passed down to torch.nn.Transformer , as well as warm.engine.forward . return: Tensor ; Same shape as y , if num_decoder > 0. Otherwise same shape as x .","title":"transformer"},{"location":"reference/warm/module/","text":"Module warm.module Custom modules to enhance the nn Sequential experience. PyWarm's core concept is to use a functional interface to simplify network building. However, if you still prefer the classical way of defining child modules in __init__() , PyWarm provides some utilities to help organize child modules better. Lambda can be used to wrap one line data transformations, like x.view() , x.permute() etc, into modules. Sequential is an extension to nn.Sequential that better accomodates PyTorch RNNs. Shortcut is another extension to nn.Sequential that will also perform a shortcut addition (AKA residual connection) for the input with output, so that residual blocks can be written in an entire sequential way. For example, to define the basic block type for resnet: import torch.nn as nn import warm.module as wm def basic_block ( size_in , size_out , stride = 1 ): block = wm . Shortcut ( nn . Conv2d ( size_in , size_out , 3 , stride , 1 , bias = False ), nn . BatchNorm2d ( size_out ), nn . ReLU (), nn . Conv2d ( size_out , size_out , 3 , 1 , 1 , bias = False ), nn . BatchNorm2d ( size_out ), projection = wm . Lambda ( lambda x : x if x . shape [ 1 ] == size_out else nn . Sequential ( nn . Conv2d ( size_in , size_out , 1 , stride , bias = False ), nn . BatchNorm2d ( size_out ), )( x ), ), ) return block Classes Lambda def : fn , * arg , ** kw Wraps a callable and all its call arguments. fn: callable ; The callable being wrapped. *arg: list ; Arguments to be passed to fn . **kw: dict ; KWargs to be passed to fn . Ancestors (in MRO) torch.nn.modules.module.Module Methods forward def : self , x forward. Sequential def : * args Similar to nn.Sequential , except that child modules can have multiple outputs (e.g. nn.RNN ). *arg: list of Modules ; Same as nn.Sequential . Ancestors (in MRO) torch.nn.modules.container.Sequential torch.nn.modules.module.Module Descendants warm.module.Shortcut Methods forward def : self , x forward. Shortcut def : * arg , projection = None Similar to nn.Sequential , except that it performs a shortcut addition for the input and output. *arg: list of Modules ; Same as nn.Sequential . projection: None or callable ; If None , input with be added directly to the output. otherwise input will be passed to the projection first, usually to make the shapes match. Ancestors (in MRO) warm.module.Sequential torch.nn.modules.container.Sequential torch.nn.modules.module.Module","title":"Module"},{"location":"reference/warm/module/#module-warmmodule","text":"Custom modules to enhance the nn Sequential experience. PyWarm's core concept is to use a functional interface to simplify network building. However, if you still prefer the classical way of defining child modules in __init__() , PyWarm provides some utilities to help organize child modules better. Lambda can be used to wrap one line data transformations, like x.view() , x.permute() etc, into modules. Sequential is an extension to nn.Sequential that better accomodates PyTorch RNNs. Shortcut is another extension to nn.Sequential that will also perform a shortcut addition (AKA residual connection) for the input with output, so that residual blocks can be written in an entire sequential way. For example, to define the basic block type for resnet: import torch.nn as nn import warm.module as wm def basic_block ( size_in , size_out , stride = 1 ): block = wm . Shortcut ( nn . Conv2d ( size_in , size_out , 3 , stride , 1 , bias = False ), nn . BatchNorm2d ( size_out ), nn . ReLU (), nn . Conv2d ( size_out , size_out , 3 , 1 , 1 , bias = False ), nn . BatchNorm2d ( size_out ), projection = wm . Lambda ( lambda x : x if x . shape [ 1 ] == size_out else nn . Sequential ( nn . Conv2d ( size_in , size_out , 1 , stride , bias = False ), nn . BatchNorm2d ( size_out ), )( x ), ), ) return block","title":"Module warm.module"},{"location":"reference/warm/module/#classes","text":"","title":"Classes"},{"location":"reference/warm/module/#lambda","text":"def : fn , * arg , ** kw Wraps a callable and all its call arguments. fn: callable ; The callable being wrapped. *arg: list ; Arguments to be passed to fn . **kw: dict ; KWargs to be passed to fn .","title":"Lambda"},{"location":"reference/warm/module/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/warm/module/#methods","text":"","title":"Methods"},{"location":"reference/warm/module/#forward","text":"def : self , x forward.","title":"forward"},{"location":"reference/warm/module/#sequential","text":"def : * args Similar to nn.Sequential , except that child modules can have multiple outputs (e.g. nn.RNN ). *arg: list of Modules ; Same as nn.Sequential .","title":"Sequential"},{"location":"reference/warm/module/#ancestors-in-mro_1","text":"torch.nn.modules.container.Sequential torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/warm/module/#descendants","text":"warm.module.Shortcut","title":"Descendants"},{"location":"reference/warm/module/#methods_1","text":"","title":"Methods"},{"location":"reference/warm/module/#forward_1","text":"def : self , x forward.","title":"forward"},{"location":"reference/warm/module/#shortcut","text":"def : * arg , projection = None Similar to nn.Sequential , except that it performs a shortcut addition for the input and output. *arg: list of Modules ; Same as nn.Sequential . projection: None or callable ; If None , input with be added directly to the output. otherwise input will be passed to the projection first, usually to make the shapes match.","title":"Shortcut"},{"location":"reference/warm/module/#ancestors-in-mro_2","text":"warm.module.Sequential torch.nn.modules.container.Sequential torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/warm/util/","text":"Module warm.util Short utilities. Functions camel_to_snake def : name Convert a camelCaseString to its snake_case_equivalent. summary def : model Print a summary about model building blocks and parameter counts. summary_str def : model Get a string representation of model building blocks and parameter counts.","title":"Util"},{"location":"reference/warm/util/#module-warmutil","text":"Short utilities.","title":"Module warm.util"},{"location":"reference/warm/util/#functions","text":"","title":"Functions"},{"location":"reference/warm/util/#camel_to_snake","text":"def : name Convert a camelCaseString to its snake_case_equivalent.","title":"camel_to_snake"},{"location":"reference/warm/util/#summary","text":"def : model Print a summary about model building blocks and parameter counts.","title":"summary"},{"location":"reference/warm/util/#summary_str","text":"def : model Get a string representation of model building blocks and parameter counts.","title":"summary_str"}]}